{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d0bcede-0826-475c-8678-72835c042b37",
   "metadata": {},
   "source": [
    "# Customer Purchase Prediction\n",
    "\n",
    "RetailTech Solutions is a fast-growing international e-commerce platform operating in over 20 countries across Europe, North America, and Asia. They specialize in fashion, electronics, and home goods, with a unique business model that combines traditional retail with a marketplace for independent sellers.\n",
    "\n",
    "The company has seen rapid growth. A key part of their success has been their data-driven approach to personalization. However, as they plan their expansion into new markets, they need to improve their ability to predict customer behavior.\n",
    "\n",
    "Their marketing team wants to predict which customers are most likely to make a purchase based on their browsing behavior.\n",
    "\n",
    "As an AI Engineer, you will help build this prediction system. Your work will directly impact RetailTech's growth strategy and their goal of increasing revenue.\n",
    "\n",
    "\n",
    "## Data Description\n",
    "\n",
    "| Column Name | Criteria |\n",
    "|------------|----------|\n",
    "| customer_id | Integer. Unique identifier for each customer. No missing values. |\n",
    "| time_spent | Float. Minutes spent on website per session. Missing values should be replaced with median. |\n",
    "| pages_viewed | Integer. Number of pages viewed in session. Missing values should be replaced with mean. |\n",
    "| basket_value | Float. Value of items in basket. Missing values should be replaced with 0. |\n",
    "| device_type | String. One of: Mobile, Desktop, Tablet. Missing values should be replaced with \"Unknown\". |\n",
    "| customer_type | String. One of: New, Returning. Missing values should be replaced with \"New\". |\n",
    "| purchase | Binary. Whether customer made a purchase (1) or not (0). Target variable. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d5a3bb-bbae-4d39-a6c6-daa46c470347",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "The marketing team has collected customer session data in `raw_customer_data.csv`, but it contains missing values and inconsistencies that need to be addressed.\n",
    "Create a cleaned version of the dataframe:\n",
    "\n",
    "- Start with the data in the file `raw_customer_data.csv`\n",
    "- Your output should be a DataFrame named `clean_data`\n",
    "- All column names and values should match the table below.\n",
    "</br>\n",
    "\n",
    "| Column Name | Criteria |\n",
    "|------------|----------|\n",
    "| customer_id | Integer. Unique identifier for each customer. No missing values. |\n",
    "| time_spent | Float. Minutes spent on website per session. Missing values should be replaced with median. |\n",
    "| pages_viewed | Integer. Number of pages viewed in session. Missing values should be replaced with mean. |\n",
    "| basket_value | Float. Value of items in basket. Missing values should be replaced with 0. |\n",
    "| device_type | String. One of: Mobile, Desktop, Tablet. Missing values should be replaced with \"Unknown\". |\n",
    "| customer_type | String. One of: New, Returning. Missing values should be replaced with \"New\". |\n",
    "| purchase | Binary. Whether customer made a purchase (1) or not (0). Target variable. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ce18b54-29af-4beb-bc8c-79c4e21bcd52",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 55,
    "lastExecutedAt": 1749960081957,
    "lastExecutedByKernel": "da99be78-980a-4107-8c12-e89c5f8aed09",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import pandas as pd\n\n# Step 1: Load the dataset\ndf = pd.read_csv('raw_customer_data.csv')\n\n# Step 2: Clean missing values\ndf['time_spent'].fillna(df['time_spent'].median(), inplace=True)\ndf['pages_viewed'].fillna(df['pages_viewed'].mean(), inplace=True)\ndf['basket_value'].fillna(0, inplace=True)\ndf['device_type'].fillna('Unknown', inplace=True)\ndf['customer_type'].fillna('New', inplace=True)\n\n# Step 3: Ensure correct data types\ndf['customer_id'] = df['customer_id'].astype(int)\ndf['pages_viewed'] = df['pages_viewed'].astype(int)\ndf['purchase'] = df['purchase'].astype(int)\n\n# Step 4: Store cleaned data\nclean_data = df",
    "outputsMetadata": {
     "0": {
      "height": 332,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "df = pd.read_csv('raw_customer_data.csv')\n",
    "\n",
    "# Step 2: Clean missing values\n",
    "df['time_spent'].fillna(df['time_spent'].median(), inplace=True)\n",
    "df['pages_viewed'].fillna(df['pages_viewed'].mean(), inplace=True)\n",
    "df['basket_value'].fillna(0, inplace=True)\n",
    "df['device_type'].fillna('Unknown', inplace=True)\n",
    "df['customer_type'].fillna('New', inplace=True)\n",
    "\n",
    "# Step 3: Ensure correct data types\n",
    "df['customer_id'] = df['customer_id'].astype(int)\n",
    "df['pages_viewed'] = df['pages_viewed'].astype(int)\n",
    "df['purchase'] = df['purchase'].astype(int)\n",
    "\n",
    "# Step 4: Store cleaned data\n",
    "clean_data = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026b3c30-d3b0-4762-ae10-0f2880873bdc",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "The pre-cleaned dataset `model_data.csv` needs to be prepared for our neural network.\n",
    "Create the model features:\n",
    "\n",
    "- Start with the data in the file `model_data.csv`\n",
    "- Scale numerical features (`time_spent`, `pages_viewed`, `basket_value`) to 0-1 range\n",
    "- Apply one-hot encoding to the categorical features (`device_type`, `customer_type`)\n",
    "    - The column names should have the following format: variable_name_category_name (e.g., `device_type_Desktop`)\n",
    "- Your output should be a DataFrame named `model_feature_set`, with all column names from `model_data.csv` except for the columns where one-hot encoding was applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d47e440-c4ab-45cf-af40-53181764bac4",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 48,
    "lastExecutedAt": 1749960082005,
    "lastExecutedByKernel": "da99be78-980a-4107-8c12-e89c5f8aed09",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Step 1: Load the data\ndf = pd.read_csv('model_data.csv')\n\n# Step 2: Scale numerical features\nscaler = MinMaxScaler()\nnumerical_cols = ['time_spent', 'pages_viewed', 'basket_value']\ndf[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n\n# Step 3: One-hot encode categorical features\ncategorical_cols = ['device_type', 'customer_type']\ndf_encoded = pd.get_dummies(df[categorical_cols], prefix=categorical_cols)\n\n# Step 4: Drop original categorical columns and concatenate encoded columns\ndf.drop(columns=categorical_cols, inplace=True)\nmodel_feature_set = pd.concat([df, df_encoded], axis=1)",
    "outputsMetadata": {
     "0": {
      "height": 332,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Step 1: Load the data\n",
    "df = pd.read_csv('model_data.csv')\n",
    "\n",
    "# Step 2: Scale numerical features\n",
    "scaler = MinMaxScaler()\n",
    "numerical_cols = ['time_spent', 'pages_viewed', 'basket_value']\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "# Step 3: One-hot encode categorical features\n",
    "categorical_cols = ['device_type', 'customer_type']\n",
    "df_encoded = pd.get_dummies(df[categorical_cols], prefix=categorical_cols)\n",
    "\n",
    "# Step 4: Drop original categorical columns and concatenate encoded columns\n",
    "df.drop(columns=categorical_cols, inplace=True)\n",
    "model_feature_set = pd.concat([df, df_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a02327-d528-441c-87bf-098f9d6415e1",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "Now that all preparatory work has been done, create and train a neural network that would allow the company to predict purchases.\n",
    "\n",
    "- Using PyTorch, create a network with:\n",
    "   - At least one hidden layer with 8 units\n",
    "   - ReLU activation for hidden layer\n",
    "   - Sigmoid activation for the output layer\n",
    "- Using the prepared features in `input_model_features.csv`, train the model to predict purchases. \n",
    "- Use the validation dataset `validation_features.csv` to predict new values based on the trained model. \n",
    "- Your model should be named `purchase_model` and your output should be a DataFrame named `validation_predictions` with columns `customer_id` and `purchase`. The `purchase` column must be your predicted values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efcbda28-3c89-480d-b77a-c7f27ac759d5",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 207,
    "lastExecutedAt": 1749960082212,
    "lastExecutedByKernel": "da99be78-980a-4107-8c12-e89c5f8aed09",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "import pandas as pd\nimport torch\nimport torch.nn as nn\nfrom sklearn.model_selection import train_test_split\n\n# Step 1: Load the datasets\ntrain_df = pd.read_csv('input_model_features.csv')\nval_df = pd.read_csv('validation_features.csv')\n\n# Step 2: Prepare features and labels\nX_train = train_df.drop(['customer_id', 'purchase'], axis=1).values\ny_train = train_df['purchase'].values\n\nX_val = val_df.drop(['customer_id'], axis=1).values\nval_customer_ids = val_df['customer_id'].values\n\n# Convert to PyTorch tensors\nX_train_tensor = torch.tensor(X_train, dtype=torch.float32)\ny_train_tensor = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n\nX_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n\n# Step 3: Define the neural network\nclass PurchaseNet(nn.Module):\n    def __init__(self, input_dim):\n        super(PurchaseNet, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 8)\n        self.relu = nn.ReLU()\n        self.output = nn.Linear(8, 1)\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.sigmoid(self.output(x))\n        return x\n\n# Initialize model\ninput_dim = X_train.shape[1]\npurchase_model = PurchaseNet(input_dim)\n\n# Step 4: Define loss and optimizer\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(purchase_model.parameters(), lr=0.01)\n\n# Step 5: Train the model\nepochs = 100\nfor epoch in range(epochs):\n    purchase_model.train()\n    optimizer.zero_grad()\n    outputs = purchase_model(X_train_tensor)\n    loss = criterion(outputs, y_train_tensor)\n    loss.backward()\n    optimizer.step()\n\n# Step 6: Predict on validation data\npurchase_model.eval()\nwith torch.no_grad():\n    predictions = purchase_model(X_val_tensor)\n    predicted_labels = (predictions.numpy() > 0.5).astype(int).flatten()\n\n# Step 7: Create validation_predictions DataFrame\nvalidation_predictions = pd.DataFrame({\n    'customer_id': val_customer_ids,\n    'purchase': predicted_labels\n})",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 1: Load the datasets\n",
    "train_df = pd.read_csv('input_model_features.csv')\n",
    "val_df = pd.read_csv('validation_features.csv')\n",
    "\n",
    "# Step 2: Prepare features and labels\n",
    "X_train = train_df.drop(['customer_id', 'purchase'], axis=1).values\n",
    "y_train = train_df['purchase'].values\n",
    "\n",
    "X_val = val_df.drop(['customer_id'], axis=1).values\n",
    "val_customer_ids = val_df['customer_id'].values\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "\n",
    "# Step 3: Define the neural network\n",
    "class PurchaseNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(PurchaseNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 8)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.output = nn.Linear(8, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X_train.shape[1]\n",
    "purchase_model = PurchaseNet(input_dim)\n",
    "\n",
    "# Step 4: Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(purchase_model.parameters(), lr=0.01)\n",
    "\n",
    "# Step 5: Train the model\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    purchase_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = purchase_model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Step 6: Predict on validation data\n",
    "purchase_model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = purchase_model(X_val_tensor)\n",
    "    predicted_labels = (predictions.numpy() > 0.5).astype(int).flatten()\n",
    "\n",
    "# Step 7: Create validation_predictions DataFrame\n",
    "validation_predictions = pd.DataFrame({\n",
    "    'customer_id': val_customer_ids,\n",
    "    'purchase': predicted_labels\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
